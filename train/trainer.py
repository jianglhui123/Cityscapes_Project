import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import os
import time
from tqdm import tqdm

from utils.metrics import calculate_iou, calculate_pixel_accuracy
from .optimizer import create_optimizer, create_scheduler
from torch.cuda.amp import autocast, GradScaler

from utils.logger import get_logger

class Trainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æœæä¾›äº†ï¼‰
        if 'seed' in config:
            import random
            import numpy as np
            import torch
            
            seed = config['seed']
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            print(f"Trainerå†…éƒ¨éšæœºç§å­: {seed}")

        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = get_logger()

        # ç¡®ä¿é…ç½®å‚æ•°æ˜¯æ­£ç¡®ç±»å‹
        self._sanitize_config()
        
        # è®°å½•é…ç½®ä¿¡æ¯
        self.logger.info("=" * 60)
        self.logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–")
        self.logger.info("=" * 60)

        # è®¾å¤‡è®¾ç½®
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        else:
            self.device = torch.device('cpu')
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.model = self.model.to(self.device)     # å°†æ•´ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆåŒ…æ‹¬æ‰€æœ‰å±‚ã€å‚æ•°ã€ç¼“å†²åŒºï¼‰ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¡ç®—è®¾å¤‡ä¸Šã€‚

        # è®°å½•æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        self.logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}") 

        # æŸå¤±å‡½æ•° - å¿½ç•¥255ï¼ˆignoreç±»ï¼‰
        self.criterion = nn.CrossEntropyLoss(ignore_index=255)
        
        # æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
        self.use_amp = config.get('use_amp', True) and torch.cuda.is_available()
        if self.use_amp:
            self.scaler = GradScaler()
            self.logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        else:
            self.scaler = None
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨æ–°çš„å‡½æ•°ï¼‰
        self.optimizer = create_optimizer(model, config)
        
        # åˆ›å»ºè°ƒåº¦å™¨ï¼ˆéœ€è¦çŸ¥é“æ¯ä¸ªepochçš„æ­¥æ•°ï¼‰
        self.scheduler_type = config.get('scheduler', 'plateau')
        if self.scheduler_type == 'onecycle':
            # OneCycleLRéœ€è¦åœ¨è®­ç»ƒå¼€å§‹å‰çŸ¥é“steps_per_epoch
            self.steps_per_epoch = len(train_loader)
            self.scheduler = create_scheduler(
                self.optimizer, 
                config, 
                steps_per_epoch=self.steps_per_epoch
            )
        else:
            self.scheduler = create_scheduler(self.optimizer, config)
        
        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_mious = []
        self.best_miou = 0
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(config['save_dir'], exist_ok=True)
        os.makedirs(os.path.join(config['save_dir'], 'weights'), exist_ok=True)
        os.makedirs(os.path.join(config['save_dir'], 'plots'), exist_ok=True)
        os.makedirs(os.path.join(config['save_dir'], 'logs'), exist_ok=True)

        self.logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info("-" * 60)
    
    def _sanitize_config(self):
        """æ¸…ç†é…ç½®å‚æ•°ï¼Œç¡®ä¿ç±»å‹æ­£ç¡®"""
        # ç¡®ä¿å¿…è¦çš„é”®å­˜åœ¨
        required_keys = ['learning_rate', 'weight_decay', 'batch_size', 'num_epochs']
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"é…ç½®ä¸­ç¼ºå°‘å¿…è¦çš„é”®: {key}")
        
        # è½¬æ¢ç±»å‹
        self.config['learning_rate'] = float(self.config['learning_rate'])
        self.config['weight_decay'] = float(self.config['weight_decay'])
        self.config['batch_size'] = int(self.config['batch_size'])
        self.config['num_epochs'] = int(self.config['num_epochs'])
        
        # æ·»åŠ é»˜è®¤å€¼ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.config.setdefault('save_dir', './results')
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰"""
        self.model.train()
        total_loss = 0
        progress_bar = tqdm(self.train_loader, desc='è®­ç»ƒ')
        
        for batch_idx, (images, labels) in enumerate(progress_bar):
            images = images.to(self.device)
            
            # ç¡®ä¿æ ‡ç­¾ä¸ºlongç±»å‹
            if labels.dtype != torch.long:
                labels = labels.long()
            labels = labels.to(self.device)
            
            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            if self.use_amp and self.scaler is not None:
                # æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast():
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                
                # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # æ™®é€šè®­ç»ƒ
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
            
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆå¯¹äºOneCycleLRï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦æ›´æ–°ï¼‰
            if self.scheduler_type == 'onecycle':
                self.scheduler.step()
            
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()   # å°†ç¥ç»ç½‘ç»œåˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼
        total_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            progress_bar = tqdm(self.val_loader, desc='éªŒè¯')
            for images, labels in progress_bar:
                images = images.to(self.device)
                
                # ç¡®ä¿æ ‡ç­¾ä¸ºlongç±»å‹
                if labels.dtype != torch.long:
                    labels = labels.long()
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                total_loss += loss.item()
                
                # è·å–é¢„æµ‹
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.append(preds.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels)
        
        miou = calculate_iou(all_preds, all_labels, num_classes=19, ignore_index=255)
        pixel_acc = calculate_pixel_accuracy(all_preds, all_labels, ignore_index=255)
        
        avg_loss = total_loss / len(self.val_loader)
        
        return avg_loss, miou, pixel_acc
    
    def train(self):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹è®­ç»ƒ")
        self.logger.info("=" * 60)
        self.logger.info(f"è®­ç»ƒepochæ•°: {self.config['num_epochs']}")
        self.logger.info(f"è®¾å¤‡: {self.device}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
        self.logger.info(f"å­¦ä¹ ç‡: {self.config['learning_rate']}")
        self.logger.info(f"æƒé‡è¡°å‡: {self.config['weight_decay']}")
        
        start_time = time.time()

        for epoch in range(self.config['num_epochs']):
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"Epoch {epoch+1}/{self.config['num_epochs']}")
            self.logger.info(f"{'='*50}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss, miou, pixel_acc = self.validate()
            self.val_losses.append(val_loss)
            self.val_mious.append(miou)
            
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆå¯¹äºéOneCycleLRè°ƒåº¦å™¨ï¼‰
            if self.scheduler_type != 'onecycle':
                if self.scheduler_type == 'plateau':
                    self.scheduler.step(miou)  # åŸºäºéªŒè¯æŒ‡æ ‡æ›´æ–°
                else:
                    self.scheduler.step()  # åŸºäºepochæ›´æ–°
            
            # è®°å½•ç»“æœ
            self.logger.info(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            self.logger.info(f"éªŒè¯æŸå¤±: {val_loss:.6f}")
            self.logger.info(f"éªŒè¯mIoU: {miou:.6f}")
            self.logger.info(f"åƒç´ å‡†ç¡®ç‡: {pixel_acc:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if miou > self.best_miou:
                self.best_miou = miou
                save_path = os.path.join(self.config['save_dir'], 'weights', 'best_model.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'miou': miou,
                    'config': self.config
                }, save_path)
                self.logger.info(f"âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒmIoU: {miou:.6f} âœ¨")
                self.logger.info(f"ä¿å­˜è·¯å¾„: {save_path}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                save_path = os.path.join(self.config['save_dir'], 'weights', f'checkpoint_epoch_{epoch+1}.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'miou': miou,
                    'config': self.config
                }, save_path)
                self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path}")
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = int(total_time % 60)
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info("è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {hours:02d}:{minutes:02d}:{seconds:02d}")
        self.logger.info(f"æœ€ä½³mIoU: {self.best_miou:.6f}")
        self.logger.info(f"{'='*60}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(self.config['save_dir'], 'weights', 'final_model.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'best_miou': self.best_miou
        }, final_path)
        self.logger.info(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {final_path}")
        
        return self.train_losses, self.val_losses, self.val_mious